{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "from graphs.llama_graph import TransformerEncoderGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
      "  (layers): ModuleList(\n",
      "    (0-29): 30 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "        (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "        (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TransformerEncoderGraph.__init__() missing 1 required positional argument: 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# defaults merge the minimum so we need to add flags or change defaults\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# example = qk=True, merge_type='all', classifier=??\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m graph1 \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoderGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgraphify()\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerEncoderGraph.__init__() missing 1 required positional argument: 'modules'"
     ]
    }
   ],
   "source": [
    "# this is a model\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "# not used yet\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# defaults merge the minimum so we need to add flags or change defaults\n",
    "# example = qk=True, merge_type='all', classifier=??\n",
    "graph1 = TransformerEncoderGraph(deepcopy(model)).graphify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp.up_proj.weight', 'self_attn.v_proj.weight', 'mlp.gate_proj.weight', 'embed_tokens.weight', 'self_attn.q_proj.weight', 'norm.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight', 'self_attn.o_proj.weight', 'mlp.down_proj.weight', 'self_attn.k_proj.weight'}\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "for name,param in model.named_parameters():\n",
    "    tmp_name = name\n",
    "    if \"layers.\" in name:\n",
    "        # find the index of the second '.'\n",
    "        second_dot_index = name.find('.', name.find('.') + 1)\n",
    "        tmp_name = name[second_dot_index+1:]\n",
    "    params.append(tmp_name)\n",
    "\n",
    "unique_params = set(params)\n",
    "print(unique_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.up_proj.weight\n",
      "self_attn.v_proj.weight\n",
      "mlp.gate_proj.weight\n",
      "embed_tokens.weight\n",
      "self_attn.q_proj.weight\n",
      "norm.weight\n",
      "input_layernorm.weight\n",
      "post_attention_layernorm.weight\n",
      "self_attn.o_proj.weight\n",
      "mlp.down_proj.weight\n",
      "self_attn.k_proj.weight\n"
     ]
    }
   ],
   "source": [
    "for param in unique_params:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
      "  (layers): ModuleList(\n",
      "    (0-29): 30 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "        (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "        (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Embedding(49152, 576, padding_idx=2)\n",
      "ModuleList(\n",
      "  (0-29): 30 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "      (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "      (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "      (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "      (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "      (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "    (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "    (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      ")\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "  (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=192, bias=False)\n",
      "Linear(in_features=576, out_features=576, bias=False)\n",
      "LlamaRotaryEmbedding()\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "  (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=576, out_features=1536, bias=False)\n",
      "Linear(in_features=1536, out_features=576, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRMSNorm((576,), eps=1e-05)\n",
      "LlamaRotaryEmbedding()\n"
     ]
    }
   ],
   "source": [
    "for module in model.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_dynamic_frequency_update',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'attention_scaling',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'config',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'inv_freq',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'max_seq_len_cached',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'original_inv_freq',\n",
       " 'original_max_seq_len',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'rope_init_fn',\n",
       " 'rope_kwargs',\n",
       " 'rope_type',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LlamaRotaryEmbedding()\n"
     ]
    }
   ],
   "source": [
    "for name, tmp_module in module.named_modules():\n",
    "    print(name)\n",
    "    print(tmp_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaRotaryEmbedding()\n"
     ]
    }
   ],
   "source": [
    "for x in module.modules():\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph1 \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoderGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pretrain_merge/merging-text-transformers/graphs/llama_graph.py:189\u001b[0m, in \u001b[0;36mTransformerEncoderGraph.graphify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m input_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(node_type\u001b[38;5;241m=\u001b[39mNodeType\u001b[38;5;241m.\u001b[39mINPUT)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# input_node -> emb_tok \u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m emb_name \u001b[38;5;241m=\u001b[39m \u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    190\u001b[0m emb_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(node_type\u001b[38;5;241m=\u001b[39mNodeType\u001b[38;5;241m.\u001b[39mEMBEDDING, \n\u001b[1;32m    191\u001b[0m                             layer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    192\u001b[0m                             param_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_directed_edge(input_node, emb_node)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'emb'"
     ]
    }
   ],
   "source": [
    "graph1 = TransformerEncoderGraph(deepcopy(model), modules={}).graphify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embed_tokens\n",
      "layers\n",
      "layers.0\n",
      "layers.0.self_attn\n",
      "layers.0.self_attn.q_proj\n",
      "layers.0.self_attn.k_proj\n",
      "layers.0.self_attn.v_proj\n",
      "layers.0.self_attn.o_proj\n",
      "layers.0.self_attn.rotary_emb\n",
      "layers.0.mlp\n",
      "layers.0.mlp.gate_proj\n",
      "layers.0.mlp.up_proj\n",
      "layers.0.mlp.down_proj\n",
      "layers.0.mlp.act_fn\n",
      "layers.0.input_layernorm\n",
      "layers.0.post_attention_layernorm\n",
      "layers.1\n",
      "layers.1.self_attn\n",
      "layers.1.self_attn.q_proj\n",
      "layers.1.self_attn.k_proj\n",
      "layers.1.self_attn.v_proj\n",
      "layers.1.self_attn.o_proj\n",
      "layers.1.self_attn.rotary_emb\n",
      "layers.1.mlp\n",
      "layers.1.mlp.gate_proj\n",
      "layers.1.mlp.up_proj\n",
      "layers.1.mlp.down_proj\n",
      "layers.1.mlp.act_fn\n",
      "layers.1.input_layernorm\n",
      "layers.1.post_attention_layernorm\n",
      "layers.2\n",
      "layers.2.self_attn\n",
      "layers.2.self_attn.q_proj\n",
      "layers.2.self_attn.k_proj\n",
      "layers.2.self_attn.v_proj\n",
      "layers.2.self_attn.o_proj\n",
      "layers.2.self_attn.rotary_emb\n",
      "layers.2.mlp\n",
      "layers.2.mlp.gate_proj\n",
      "layers.2.mlp.up_proj\n",
      "layers.2.mlp.down_proj\n",
      "layers.2.mlp.act_fn\n",
      "layers.2.input_layernorm\n",
      "layers.2.post_attention_layernorm\n",
      "layers.3\n",
      "layers.3.self_attn\n",
      "layers.3.self_attn.q_proj\n",
      "layers.3.self_attn.k_proj\n",
      "layers.3.self_attn.v_proj\n",
      "layers.3.self_attn.o_proj\n",
      "layers.3.self_attn.rotary_emb\n",
      "layers.3.mlp\n",
      "layers.3.mlp.gate_proj\n",
      "layers.3.mlp.up_proj\n",
      "layers.3.mlp.down_proj\n",
      "layers.3.mlp.act_fn\n",
      "layers.3.input_layernorm\n",
      "layers.3.post_attention_layernorm\n",
      "layers.4\n",
      "layers.4.self_attn\n",
      "layers.4.self_attn.q_proj\n",
      "layers.4.self_attn.k_proj\n",
      "layers.4.self_attn.v_proj\n",
      "layers.4.self_attn.o_proj\n",
      "layers.4.self_attn.rotary_emb\n",
      "layers.4.mlp\n",
      "layers.4.mlp.gate_proj\n",
      "layers.4.mlp.up_proj\n",
      "layers.4.mlp.down_proj\n",
      "layers.4.mlp.act_fn\n",
      "layers.4.input_layernorm\n",
      "layers.4.post_attention_layernorm\n",
      "layers.5\n",
      "layers.5.self_attn\n",
      "layers.5.self_attn.q_proj\n",
      "layers.5.self_attn.k_proj\n",
      "layers.5.self_attn.v_proj\n",
      "layers.5.self_attn.o_proj\n",
      "layers.5.self_attn.rotary_emb\n",
      "layers.5.mlp\n",
      "layers.5.mlp.gate_proj\n",
      "layers.5.mlp.up_proj\n",
      "layers.5.mlp.down_proj\n",
      "layers.5.mlp.act_fn\n",
      "layers.5.input_layernorm\n",
      "layers.5.post_attention_layernorm\n",
      "layers.6\n",
      "layers.6.self_attn\n",
      "layers.6.self_attn.q_proj\n",
      "layers.6.self_attn.k_proj\n",
      "layers.6.self_attn.v_proj\n",
      "layers.6.self_attn.o_proj\n",
      "layers.6.self_attn.rotary_emb\n",
      "layers.6.mlp\n",
      "layers.6.mlp.gate_proj\n",
      "layers.6.mlp.up_proj\n",
      "layers.6.mlp.down_proj\n",
      "layers.6.mlp.act_fn\n",
      "layers.6.input_layernorm\n",
      "layers.6.post_attention_layernorm\n",
      "layers.7\n",
      "layers.7.self_attn\n",
      "layers.7.self_attn.q_proj\n",
      "layers.7.self_attn.k_proj\n",
      "layers.7.self_attn.v_proj\n",
      "layers.7.self_attn.o_proj\n",
      "layers.7.self_attn.rotary_emb\n",
      "layers.7.mlp\n",
      "layers.7.mlp.gate_proj\n",
      "layers.7.mlp.up_proj\n",
      "layers.7.mlp.down_proj\n",
      "layers.7.mlp.act_fn\n",
      "layers.7.input_layernorm\n",
      "layers.7.post_attention_layernorm\n",
      "layers.8\n",
      "layers.8.self_attn\n",
      "layers.8.self_attn.q_proj\n",
      "layers.8.self_attn.k_proj\n",
      "layers.8.self_attn.v_proj\n",
      "layers.8.self_attn.o_proj\n",
      "layers.8.self_attn.rotary_emb\n",
      "layers.8.mlp\n",
      "layers.8.mlp.gate_proj\n",
      "layers.8.mlp.up_proj\n",
      "layers.8.mlp.down_proj\n",
      "layers.8.mlp.act_fn\n",
      "layers.8.input_layernorm\n",
      "layers.8.post_attention_layernorm\n",
      "layers.9\n",
      "layers.9.self_attn\n",
      "layers.9.self_attn.q_proj\n",
      "layers.9.self_attn.k_proj\n",
      "layers.9.self_attn.v_proj\n",
      "layers.9.self_attn.o_proj\n",
      "layers.9.self_attn.rotary_emb\n",
      "layers.9.mlp\n",
      "layers.9.mlp.gate_proj\n",
      "layers.9.mlp.up_proj\n",
      "layers.9.mlp.down_proj\n",
      "layers.9.mlp.act_fn\n",
      "layers.9.input_layernorm\n",
      "layers.9.post_attention_layernorm\n",
      "layers.10\n",
      "layers.10.self_attn\n",
      "layers.10.self_attn.q_proj\n",
      "layers.10.self_attn.k_proj\n",
      "layers.10.self_attn.v_proj\n",
      "layers.10.self_attn.o_proj\n",
      "layers.10.self_attn.rotary_emb\n",
      "layers.10.mlp\n",
      "layers.10.mlp.gate_proj\n",
      "layers.10.mlp.up_proj\n",
      "layers.10.mlp.down_proj\n",
      "layers.10.mlp.act_fn\n",
      "layers.10.input_layernorm\n",
      "layers.10.post_attention_layernorm\n",
      "layers.11\n",
      "layers.11.self_attn\n",
      "layers.11.self_attn.q_proj\n",
      "layers.11.self_attn.k_proj\n",
      "layers.11.self_attn.v_proj\n",
      "layers.11.self_attn.o_proj\n",
      "layers.11.self_attn.rotary_emb\n",
      "layers.11.mlp\n",
      "layers.11.mlp.gate_proj\n",
      "layers.11.mlp.up_proj\n",
      "layers.11.mlp.down_proj\n",
      "layers.11.mlp.act_fn\n",
      "layers.11.input_layernorm\n",
      "layers.11.post_attention_layernorm\n",
      "layers.12\n",
      "layers.12.self_attn\n",
      "layers.12.self_attn.q_proj\n",
      "layers.12.self_attn.k_proj\n",
      "layers.12.self_attn.v_proj\n",
      "layers.12.self_attn.o_proj\n",
      "layers.12.self_attn.rotary_emb\n",
      "layers.12.mlp\n",
      "layers.12.mlp.gate_proj\n",
      "layers.12.mlp.up_proj\n",
      "layers.12.mlp.down_proj\n",
      "layers.12.mlp.act_fn\n",
      "layers.12.input_layernorm\n",
      "layers.12.post_attention_layernorm\n",
      "layers.13\n",
      "layers.13.self_attn\n",
      "layers.13.self_attn.q_proj\n",
      "layers.13.self_attn.k_proj\n",
      "layers.13.self_attn.v_proj\n",
      "layers.13.self_attn.o_proj\n",
      "layers.13.self_attn.rotary_emb\n",
      "layers.13.mlp\n",
      "layers.13.mlp.gate_proj\n",
      "layers.13.mlp.up_proj\n",
      "layers.13.mlp.down_proj\n",
      "layers.13.mlp.act_fn\n",
      "layers.13.input_layernorm\n",
      "layers.13.post_attention_layernorm\n",
      "layers.14\n",
      "layers.14.self_attn\n",
      "layers.14.self_attn.q_proj\n",
      "layers.14.self_attn.k_proj\n",
      "layers.14.self_attn.v_proj\n",
      "layers.14.self_attn.o_proj\n",
      "layers.14.self_attn.rotary_emb\n",
      "layers.14.mlp\n",
      "layers.14.mlp.gate_proj\n",
      "layers.14.mlp.up_proj\n",
      "layers.14.mlp.down_proj\n",
      "layers.14.mlp.act_fn\n",
      "layers.14.input_layernorm\n",
      "layers.14.post_attention_layernorm\n",
      "layers.15\n",
      "layers.15.self_attn\n",
      "layers.15.self_attn.q_proj\n",
      "layers.15.self_attn.k_proj\n",
      "layers.15.self_attn.v_proj\n",
      "layers.15.self_attn.o_proj\n",
      "layers.15.self_attn.rotary_emb\n",
      "layers.15.mlp\n",
      "layers.15.mlp.gate_proj\n",
      "layers.15.mlp.up_proj\n",
      "layers.15.mlp.down_proj\n",
      "layers.15.mlp.act_fn\n",
      "layers.15.input_layernorm\n",
      "layers.15.post_attention_layernorm\n",
      "layers.16\n",
      "layers.16.self_attn\n",
      "layers.16.self_attn.q_proj\n",
      "layers.16.self_attn.k_proj\n",
      "layers.16.self_attn.v_proj\n",
      "layers.16.self_attn.o_proj\n",
      "layers.16.self_attn.rotary_emb\n",
      "layers.16.mlp\n",
      "layers.16.mlp.gate_proj\n",
      "layers.16.mlp.up_proj\n",
      "layers.16.mlp.down_proj\n",
      "layers.16.mlp.act_fn\n",
      "layers.16.input_layernorm\n",
      "layers.16.post_attention_layernorm\n",
      "layers.17\n",
      "layers.17.self_attn\n",
      "layers.17.self_attn.q_proj\n",
      "layers.17.self_attn.k_proj\n",
      "layers.17.self_attn.v_proj\n",
      "layers.17.self_attn.o_proj\n",
      "layers.17.self_attn.rotary_emb\n",
      "layers.17.mlp\n",
      "layers.17.mlp.gate_proj\n",
      "layers.17.mlp.up_proj\n",
      "layers.17.mlp.down_proj\n",
      "layers.17.mlp.act_fn\n",
      "layers.17.input_layernorm\n",
      "layers.17.post_attention_layernorm\n",
      "layers.18\n",
      "layers.18.self_attn\n",
      "layers.18.self_attn.q_proj\n",
      "layers.18.self_attn.k_proj\n",
      "layers.18.self_attn.v_proj\n",
      "layers.18.self_attn.o_proj\n",
      "layers.18.self_attn.rotary_emb\n",
      "layers.18.mlp\n",
      "layers.18.mlp.gate_proj\n",
      "layers.18.mlp.up_proj\n",
      "layers.18.mlp.down_proj\n",
      "layers.18.mlp.act_fn\n",
      "layers.18.input_layernorm\n",
      "layers.18.post_attention_layernorm\n",
      "layers.19\n",
      "layers.19.self_attn\n",
      "layers.19.self_attn.q_proj\n",
      "layers.19.self_attn.k_proj\n",
      "layers.19.self_attn.v_proj\n",
      "layers.19.self_attn.o_proj\n",
      "layers.19.self_attn.rotary_emb\n",
      "layers.19.mlp\n",
      "layers.19.mlp.gate_proj\n",
      "layers.19.mlp.up_proj\n",
      "layers.19.mlp.down_proj\n",
      "layers.19.mlp.act_fn\n",
      "layers.19.input_layernorm\n",
      "layers.19.post_attention_layernorm\n",
      "layers.20\n",
      "layers.20.self_attn\n",
      "layers.20.self_attn.q_proj\n",
      "layers.20.self_attn.k_proj\n",
      "layers.20.self_attn.v_proj\n",
      "layers.20.self_attn.o_proj\n",
      "layers.20.self_attn.rotary_emb\n",
      "layers.20.mlp\n",
      "layers.20.mlp.gate_proj\n",
      "layers.20.mlp.up_proj\n",
      "layers.20.mlp.down_proj\n",
      "layers.20.mlp.act_fn\n",
      "layers.20.input_layernorm\n",
      "layers.20.post_attention_layernorm\n",
      "layers.21\n",
      "layers.21.self_attn\n",
      "layers.21.self_attn.q_proj\n",
      "layers.21.self_attn.k_proj\n",
      "layers.21.self_attn.v_proj\n",
      "layers.21.self_attn.o_proj\n",
      "layers.21.self_attn.rotary_emb\n",
      "layers.21.mlp\n",
      "layers.21.mlp.gate_proj\n",
      "layers.21.mlp.up_proj\n",
      "layers.21.mlp.down_proj\n",
      "layers.21.mlp.act_fn\n",
      "layers.21.input_layernorm\n",
      "layers.21.post_attention_layernorm\n",
      "layers.22\n",
      "layers.22.self_attn\n",
      "layers.22.self_attn.q_proj\n",
      "layers.22.self_attn.k_proj\n",
      "layers.22.self_attn.v_proj\n",
      "layers.22.self_attn.o_proj\n",
      "layers.22.self_attn.rotary_emb\n",
      "layers.22.mlp\n",
      "layers.22.mlp.gate_proj\n",
      "layers.22.mlp.up_proj\n",
      "layers.22.mlp.down_proj\n",
      "layers.22.mlp.act_fn\n",
      "layers.22.input_layernorm\n",
      "layers.22.post_attention_layernorm\n",
      "layers.23\n",
      "layers.23.self_attn\n",
      "layers.23.self_attn.q_proj\n",
      "layers.23.self_attn.k_proj\n",
      "layers.23.self_attn.v_proj\n",
      "layers.23.self_attn.o_proj\n",
      "layers.23.self_attn.rotary_emb\n",
      "layers.23.mlp\n",
      "layers.23.mlp.gate_proj\n",
      "layers.23.mlp.up_proj\n",
      "layers.23.mlp.down_proj\n",
      "layers.23.mlp.act_fn\n",
      "layers.23.input_layernorm\n",
      "layers.23.post_attention_layernorm\n",
      "layers.24\n",
      "layers.24.self_attn\n",
      "layers.24.self_attn.q_proj\n",
      "layers.24.self_attn.k_proj\n",
      "layers.24.self_attn.v_proj\n",
      "layers.24.self_attn.o_proj\n",
      "layers.24.self_attn.rotary_emb\n",
      "layers.24.mlp\n",
      "layers.24.mlp.gate_proj\n",
      "layers.24.mlp.up_proj\n",
      "layers.24.mlp.down_proj\n",
      "layers.24.mlp.act_fn\n",
      "layers.24.input_layernorm\n",
      "layers.24.post_attention_layernorm\n",
      "layers.25\n",
      "layers.25.self_attn\n",
      "layers.25.self_attn.q_proj\n",
      "layers.25.self_attn.k_proj\n",
      "layers.25.self_attn.v_proj\n",
      "layers.25.self_attn.o_proj\n",
      "layers.25.self_attn.rotary_emb\n",
      "layers.25.mlp\n",
      "layers.25.mlp.gate_proj\n",
      "layers.25.mlp.up_proj\n",
      "layers.25.mlp.down_proj\n",
      "layers.25.mlp.act_fn\n",
      "layers.25.input_layernorm\n",
      "layers.25.post_attention_layernorm\n",
      "layers.26\n",
      "layers.26.self_attn\n",
      "layers.26.self_attn.q_proj\n",
      "layers.26.self_attn.k_proj\n",
      "layers.26.self_attn.v_proj\n",
      "layers.26.self_attn.o_proj\n",
      "layers.26.self_attn.rotary_emb\n",
      "layers.26.mlp\n",
      "layers.26.mlp.gate_proj\n",
      "layers.26.mlp.up_proj\n",
      "layers.26.mlp.down_proj\n",
      "layers.26.mlp.act_fn\n",
      "layers.26.input_layernorm\n",
      "layers.26.post_attention_layernorm\n",
      "layers.27\n",
      "layers.27.self_attn\n",
      "layers.27.self_attn.q_proj\n",
      "layers.27.self_attn.k_proj\n",
      "layers.27.self_attn.v_proj\n",
      "layers.27.self_attn.o_proj\n",
      "layers.27.self_attn.rotary_emb\n",
      "layers.27.mlp\n",
      "layers.27.mlp.gate_proj\n",
      "layers.27.mlp.up_proj\n",
      "layers.27.mlp.down_proj\n",
      "layers.27.mlp.act_fn\n",
      "layers.27.input_layernorm\n",
      "layers.27.post_attention_layernorm\n",
      "layers.28\n",
      "layers.28.self_attn\n",
      "layers.28.self_attn.q_proj\n",
      "layers.28.self_attn.k_proj\n",
      "layers.28.self_attn.v_proj\n",
      "layers.28.self_attn.o_proj\n",
      "layers.28.self_attn.rotary_emb\n",
      "layers.28.mlp\n",
      "layers.28.mlp.gate_proj\n",
      "layers.28.mlp.up_proj\n",
      "layers.28.mlp.down_proj\n",
      "layers.28.mlp.act_fn\n",
      "layers.28.input_layernorm\n",
      "layers.28.post_attention_layernorm\n",
      "layers.29\n",
      "layers.29.self_attn\n",
      "layers.29.self_attn.q_proj\n",
      "layers.29.self_attn.k_proj\n",
      "layers.29.self_attn.v_proj\n",
      "layers.29.self_attn.o_proj\n",
      "layers.29.self_attn.rotary_emb\n",
      "layers.29.mlp\n",
      "layers.29.mlp.gate_proj\n",
      "layers.29.mlp.up_proj\n",
      "layers.29.mlp.down_proj\n",
      "layers.29.mlp.act_fn\n",
      "layers.29.input_layernorm\n",
      "layers.29.post_attention_layernorm\n",
      "norm\n",
      "rotary_emb\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayers.29.mlp.up_proj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "module.named_modules()[\"layers.29.mlp.up_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_modules_dict = dict(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'in_features',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'out_features',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(named_modules_dict[\"layers.29.mlp.up_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 576])\n"
     ]
    }
   ],
   "source": [
    "for x in named_modules_dict[\"layers.29.mlp.up_proj\"].parameters():\n",
    "    print(x.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
