{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "from graphs.llama_graph import TransformerEncoderGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
      "  (layers): ModuleList(\n",
      "    (0-29): 30 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "        (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "        (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# this is a model\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "# not used yet\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# defaults merge the minimum so we need to add flags or change defaults\n",
    "# example = qk=True, merge_type='all', classifier=??\n",
    "#graph1 = TransformerEncoderGraph(deepcopy(model)).graphify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(0, 100, (1, 10))\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hook(module, input, output):\n",
    "    return output\n",
    "\n",
    "activation = {}\n",
    "model.layers[3].register_forward_hook(lambda m, i, o: activation.update({'layer3': o}))\n",
    "input_ids = torch.randint(0, 100, (1, 10))\n",
    "input_ids = input_ids.to(model.device)\n",
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_graph.png'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output.last_hidden_state\n",
    "dot = make_dot(activation['layer3'][0], params=dict(model.named_parameters()))\n",
    "dot.render(\"llama_graph\", format=\"png\", cleanup=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp.up_proj.weight', 'self_attn.v_proj.weight', 'mlp.gate_proj.weight', 'embed_tokens.weight', 'self_attn.q_proj.weight', 'norm.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight', 'self_attn.o_proj.weight', 'mlp.down_proj.weight', 'self_attn.k_proj.weight'}\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "for name,param in model.named_parameters():\n",
    "    tmp_name = name\n",
    "    if \"layers.\" in name:\n",
    "        # find the index of the second '.'\n",
    "        second_dot_index = name.find('.', name.find('.') + 1)\n",
    "        tmp_name = name[second_dot_index+1:]\n",
    "    params.append(tmp_name)\n",
    "\n",
    "unique_params = set(params)\n",
    "print(unique_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.up_proj.weight\n",
      "self_attn.v_proj.weight\n",
      "mlp.gate_proj.weight\n",
      "embed_tokens.weight\n",
      "self_attn.q_proj.weight\n",
      "norm.weight\n",
      "input_layernorm.weight\n",
      "post_attention_layernorm.weight\n",
      "self_attn.o_proj.weight\n",
      "mlp.down_proj.weight\n",
      "self_attn.k_proj.weight\n"
     ]
    }
   ],
   "source": [
    "for param in unique_params:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp.act_fn', 'norm', 'self_attn.v_proj', 'mlp.down_proj', 'embed_tokens', 'self_attn.rotary_emb', 'mlp', 'self_attn.o_proj', 'input_layernorm', 'post_attention_layernorm', 'mlp.gate_proj', 'rotary_emb', 'self_attn.q_proj', 'self_attn', 'self_attn.k_proj', 'mlp.up_proj'}\n"
     ]
    }
   ],
   "source": [
    "module_name_set = set()\n",
    "for name, module in model.named_modules():\n",
    "    if name == \"layers\":\n",
    "        continue\n",
    "    if \"layers\" in name and len(name.split(\".\")) == 2:\n",
    "        #print(\"skipping \", name)\n",
    "        continue\n",
    "    if \"layers\" in name:\n",
    "        second_dot_index = name.find('.', name.find('.') + 1)\n",
    "        tmp_name = name[second_dot_index+1:]\n",
    "        if len(tmp_name) > 0:\n",
    "            module_name_set.add(tmp_name)\n",
    "    else:\n",
    "        if name == \"\":\n",
    "            continue\n",
    "        module_name_set.add(name)\n",
    "print(module_name_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens\n",
      "input_layernorm\n",
      "mlp\n",
      "mlp.act_fn\n",
      "mlp.down_proj\n",
      "mlp.gate_proj\n",
      "mlp.up_proj\n",
      "norm\n",
      "post_attention_layernorm\n",
      "rotary_emb\n",
      "self_attn\n",
      "self_attn.k_proj\n",
      "self_attn.o_proj\n",
      "self_attn.q_proj\n",
      "self_attn.rotary_emb\n",
      "self_attn.v_proj\n"
     ]
    }
   ],
   "source": [
    "for name in sorted(list(module_name_set)):\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1 = TransformerEncoderGraph(deepcopy(model), modules={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph1 \u001b[38;5;241m=\u001b[39m \u001b[43mgraph1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/pretrain_merge/merging-text-transformers/graphs/llama_graph.py:189\u001b[0m, in \u001b[0;36mTransformerEncoderGraph.graphify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m input_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(node_type\u001b[38;5;241m=\u001b[39mNodeType\u001b[38;5;241m.\u001b[39mINPUT)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# input_node -> emb_tok \u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m emb_name \u001b[38;5;241m=\u001b[39m \u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    190\u001b[0m emb_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(node_type\u001b[38;5;241m=\u001b[39mNodeType\u001b[38;5;241m.\u001b[39mEMBEDDING, \n\u001b[1;32m    191\u001b[0m                             layer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    192\u001b[0m                             param_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_directed_edge(input_node, emb_node)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'emb'"
     ]
    }
   ],
   "source": [
    "graph1 = graph1.graphify()\n",
    "# todo: figure out when postfix and prefix are used\n",
    "# todo: implement adding embedding (trivial)\n",
    "# todo: implement llama decoder block\n",
    "# todo: repeat llama decoder block for all layers\n",
    "# todo: implment final 2 layers\n",
    "# todo: figure out what we do for any output layers\n",
    "# todo: check how to use positional embeddings if there are any after embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 576])\n"
     ]
    }
   ],
   "source": [
    "for x in named_modules_dict[\"layers.29.mlp.up_proj\"].parameters():\n",
    "    print(x.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
